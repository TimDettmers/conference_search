(lp1
S' We present a new Convolutional Neural Network (CNN) model for text\nclassification that jointly exploits labels on documents and their component\nsentences. Specifically, we consider scenarios in which annotators explicitly\nmark sentences (or snippets) that support their overall document\ncategorization, i.e., they provide rationales. Our model exploits such\nsupervision via a hierarchical approach in which each document is represented\nby a linear combination of the vector representations of its component\nsentences. We propose a sentence-level convolutional model that estimates the\nprobability that a given sentence is a rationale, and we then scale the\ncontribution of each sentence to the aggregate document representation in\nproportion to these estimates. Experiments on five classification datasets that\nhave document labels and associated rationales demonstrate that our approach\nconsistently outperforms strong baselines. Moreover, our model naturally\nprovides explanations for its predictions.\n'
p2
aS' We introduce a deep memory network for aspect level sentiment classification.\nUnlike feature-based SVM and sequential neural models such as LSTM, this\napproach explicitly captures the importance of each context word when inferring\nthe sentiment polarity of an aspect. Such importance degree and text\nrepresentation are calculated with multiple computational layers, each of which\nis a neural attention model over an external memory. Experiments on laptop and\nrestaurant datasets demonstrate that our approach performs comparable to\nstate-of-art feature based SVM system, and substantially better than LSTM and\nattention-based LSTM architectures. On both datasets we show that multiple\ncomputational layers could improve the performance. Moreover, our approach is\nalso fast. The deep memory network with 9 layers is 15 times faster than LSTM\nwith a CPU implementation.\n'
p3
aS' Language models (LMs) are statistical models that calculate probabilities\nover sequences of words or other discrete symbols. Currently two major\nparadigms for language modeling exist: count-based n-gram models, which have\nadvantages of scalability and test-time speed, and neural LMs, which often\nachieve superior modeling performance. We demonstrate how both varieties of\nmodels can be unified in a single modeling framework that defines a set of\nprobability distributions over the vocabulary of words, and then dynamically\ncalculates mixture weights over these distributions. This formulation allows us\nto create novel hybrid models that combine the desirable features of\ncount-based and neural LMs, and experiments demonstrate the advantages of these\napproaches.\n'
p4
aS' We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a\nnew structured prediction algorithm that generalizes the Collins Structured\nPerceptron (CSP). Unlike CSP, the update rule of SWVP explicitly exploits the\ninternal structure of the predicted labels. We prove the convergence of SWVP\nfor linearly separable training sets, provide mistake and generalization\nbounds, and show that in the general case these bounds are tighter than those\nof the CSP special case. In synthetic data experiments with data drawn from an\nHMM, various variants of SWVP substantially outperform its CSP special case.\nSWVP also provides encouraging initial dependency parsing results.\n'
p5
aS' Recently a variety of LSTM-based conditional language models (LM) have been\napplied across a range of language generation tasks. In this work we study\nvarious model architectures and different ways to represent and aggregate the\nsource information in an end-to-end neural dialogue system framework. A method\ncalled snapshot learning is also proposed to facilitate learning from\nsupervised sequential signals by applying a companion cross-entropy objective\nfunction to the conditioning vector. The experimental and analytical results\ndemonstrate firstly that competition occurs between the conditioning vector and\nthe LM, and the differing architectures provide different trade-offs between\nthe two. Secondly, the discriminative power and transparency of the\nconditioning vector is key to providing both model interpretability and better\nperformance. Thirdly, snapshot learning leads to consistent performance\nimprovements independent of which architecture is used.\n'
p6
aS" A word's sentiment depends on the domain in which it is used. Computational\nsocial science research thus requires sentiment lexicons that are specific to\nthe domains being studied. We combine domain-specific word embeddings with a\nlabel propagation framework to induce accurate domain-specific sentiment\nlexicons using small sets of seed words, achieving state-of-the-art performance\ncompetitive with approaches that rely on hand-curated resources. Using our\nframework we perform two large-scale empirical studies to quantify the extent\nto which sentiment varies across time and between communities. We induce and\nrelease historical sentiment lexicons for 150 years of English and\ncommunity-specific sentiment lexicons for 250 online communities from the\nsocial media forum Reddit. The historical lexicons show that more than 5% of\nsentiment-bearing (non-neutral) English words completely switched polarity\nduring the last 150 years, and the community-specific lexicons highlight how\nsentiment varies drastically between different communities.\n"
p7
aS' Modeling textual or visual information with vector representations trained\nfrom large language or visual datasets has been successfully explored in recent\nyears. However, tasks such as visual question answering require combining these\nvector representations with each other. Approaches to multimodal pooling\ninclude element-wise product or sum, as well as concatenation of the visual and\ntextual representations. We hypothesize that these methods are not as\nexpressive as an outer product of the visual and textual vectors. As the outer\nproduct is typically infeasible due to its high dimensionality, we instead\npropose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and\nexpressively combine multimodal features. We extensively evaluate MCB on the\nvisual question answering and grounding tasks. We consistently show the benefit\nof MCB over ablations without MCB. For visual question answering, we present an\narchitecture which uses MCB twice, once for predicting attention over spatial\nfeatures and again to combine the attended representation with the question\nrepresentation. This model outperforms the state-of-the-art on the Visual7W\ndataset and the VQA challenge.\n'
p8
aS' Within the field of Statistical Machine Translation (SMT), the neural\napproach (NMT) has recently emerged as the first technology able to challenge\nthe long-standing dominance of phrase-based approaches (PBMT). In particular,\nat the IWSLT 2015 evaluation campaign, NMT outperformed well established\nstate-of-the-art PBMT systems on English-German, a language pair known to be\nparticularly hard because of morphology and syntactic differences. To\nunderstand in what respects NMT provides better translation quality than PBMT,\nwe perform a detailed analysis of neural versus phrase-based SMT outputs,\nleveraging high quality post-edits performed by professional translators on the\nIWSLT data. For the first time, our analysis provides useful insights on what\nlinguistic phenomena are best modeled by neural models -- such as the\nreordering of verbs -- while pointing out other aspects that remain to be\nimproved.\n'
p9
aS' Evaluation of NLP methods requires testing against a previously vetted\ngold-standard test set and reporting standard metrics\n(accuracy/precision/recall/F1). The current assumption is that all items in a\ngiven test set are equal with regards to difficulty and discriminating power.\nWe propose Item Response Theory (IRT) from psychometrics as an alternative\nmeans for gold-standard test-set generation and NLP system evaluation. IRT is\nable to describe characteristics of individual items - their difficulty and\ndiscriminating power - and can account for these characteristics in its\nestimation of human intelligence or ability for an NLP task. In this paper, we\ndemonstrate IRT by generating a gold-standard test set for Recognizing Textual\nEntailment. By collecting a large number of human responses and fitting our IRT\nmodel, we show that our IRT model compares NLP systems with the performance in\na human population and is able to provide more insight into system performance\nthan standard evaluation metrics. We show that a high accuracy score does not\nalways imply a high IRT score, which depends on the item characteristics and\nthe response pattern.\n'
p10
aS' In this paper we address the question of how to render sequence-level\nnetworks better at handling structured input. We propose a machine reading\nsimulator which processes text incrementally from left to right and performs\nshallow reasoning with memory and attention. The reader extends the Long\nShort-Term Memory architecture with a memory network in place of a single\nmemory cell. This enables adaptive memory usage during recurrence with neural\nattention, offering a way to weakly induce relations among tokens. The system\nis initially designed to process a single sequence but we also demonstrate how\nto integrate it with an encoder-decoder architecture. Experiments on language\nmodeling, sentiment analysis, and natural language inference show that our\nmodel matches or outperforms the state of the art.\n'
p11
aS' In this paper, we present a study on personalized emphasis framing which can\nbe used to tailor the content of a message to enhance its appeal to different\nindividuals. With this framework, we directly model content selection decisions\nbased on a set of psychologically-motivated domain-independent personal traits\nincluding personality (e.g., extraversion and conscientiousness) and basic\nhuman values (e.g., self-transcendence and hedonism). We also demonstrate how\nthe analysis results can be used in automated personalized content selection\nfor persuasive message generation.\n'
p12
aS' In aspect-based sentiment analysis, extracting aspect terms along with the\nopinions being expressed from user-generated content is one of the most\nimportant subtasks. Previous studies have shown that exploiting connections\nbetween aspect and opinion terms is promising for this task. In this paper, we\npropose a novel joint model that integrates recursive neural networks and\nconditional random fields into a unified framework for explicit aspect and\nopinion terms co-extraction. The proposed model learns high-level\ndiscriminative features and double propagate information between aspect and\nopinion terms, simultaneously. Moreover, it is flexible to incorporate\nhand-crafted features into the proposed model to further boost its information\nextraction performance. Experimental results on the SemEval Challenge 2014\ndataset show the superiority of our proposed model over several baseline\nmethods as well as the winning systems of the challenge.\n'
p13
aS' Crosslingual word embeddings represent lexical items from different languages\nin the same vector space, enabling transfer of NLP tools. However, previous\nattempts had expensive resource requirements, difficulty incorporating\nmonolingual data or were unable to handle polysemy. We address these drawbacks\nin our method which takes advantage of a high coverage dictionary in an EM\nstyle training algorithm over monolingual corpora in two languages. Our model\nachieves state-of-the-art performance on bilingual lexicon induction task\nexceeding models using large bilingual corpora, and competitive results on the\nmonolingual word similarity and cross-lingual document classification task.\n'
p14
aS' In this paper we introduce Latent Tree Language Model (LTLM), a novel\napproach to language modeling that encodes syntax and semantics of a given\nsentence as a tree of word roles.\n'
p15
aS" We investigate evaluation metrics for end-to-end dialogue systems where\nsupervised labels, such as task completion, are not available. Recent works in\nend-to-end dialogue systems have adopted metrics from machine translation and\ntext summarization to compare a model's generated response to a single target\nresponse. We show that these metrics correlate very weakly or not at all with\nhuman judgements of the response quality in both technical and non-technical\ndomains. We provide quantitative and qualitative results highlighting specific\nweaknesses in existing metrics, and provide recommendations for future\ndevelopment of better automatic evaluation metrics for dialogue systems.\n"
p16
aS' This paper introduces a neural model for concept-to-text generation that\nscales to large, rich domains. We experiment with a new dataset of biographies\nfrom Wikipedia that is an order of magnitude larger than existing resources\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\nmodel builds upon recent work on conditional neural language model for text\ngeneration. To deal with the large vocabulary, we extend these models to mix a\nfixed vocabulary with copy actions that transfer sample-specific words from the\ninput database to the generated output sentence. Our neural model significantly\nout-performs a classical Kneser-Ney language model adapted to this task by\nnearly 15 BLEU.\n'
p17
aS' Models of neural machine translation are often from a discriminative family\nof encoderdecoders that learn a conditional distribution of a target sentence\ngiven a source sentence. In this paper, we propose a variational model to learn\nthis conditional distribution for neural machine translation: a variational\nencoderdecoder model that can be trained end-to-end. Different from the vanilla\nencoder-decoder model that generates target translations from hidden\nrepresentations of source sentences alone, the variational model introduces a\ncontinuous latent variable to explicitly model underlying semantics of source\nsentences and to guide the generation of target translations. In order to\nperform efficient posterior inference and large-scale training, we build a\nneural posterior approximator conditioned on both the source and the target\nsides, and equip it with a reparameterization technique to estimate the\nvariational lower bound. Experiments on both Chinese-English and English-\nGerman translation tasks show that the proposed variational neural machine\ntranslation achieves significant improvements over the vanilla neural machine\ntranslation baselines.\n'
p18
aS' A common model for question answering (QA) is that a good answer is one that\nis closely related to the question, where relatedness is often determined using\ngeneral-purpose lexical models such as word embeddings. We argue that a better\napproach is to look for answers that are related to the question in a relevant\nway, according to the information need of the question, which may be determined\nthrough task-specific embeddings. With causality as a use case, we implement\nthis insight in three steps. First, we generate causal embeddings\ncost-effectively by bootstrapping cause-effect pairs extracted from free text\nusing a small set of seed patterns. Second, we train dedicated embeddings over\nthis data, by using task-specific contexts, i.e., the context of a cause is its\neffect. Finally, we extend a state-of-the-art reranking approach for QA to\nincorporate these causal embeddings. We evaluate the causal embedding models\nboth directly with a casual implication task, and indirectly, in a downstream\ncausal QA task using data from Yahoo! Answers. We show that explicitly modeling\ncausality improves performance in both tasks. In the QA task our best model\nachieves 37.3% P@1, significantly outperforming a strong baseline by 7.7%\n(relative).\n'
p19
aS' In multilingual question answering, either the question needs to be\ntranslated into the document language, or vice versa. In addition to direction,\nthere are multiple methods to perform the translation, four of which we explore\nin this paper: word-based, 10-best, context-based, and grammar-based. We build\na feature for each combination of translation direction and method, and train a\nmodel that learns optimal feature weights. On a large forum dataset consisting\nof posts in English, Arabic, and Chinese, our novel learn-to-translate approach\nwas more effective than a strong baseline (p<0.05): translating all text into\nEnglish, then training a classifier based only on English (original or\ntranslated) text.\n'
p20
aS" We study the problem of identifying individuals based on their characteristic\ngaze patterns during reading of arbitrary text. The motivation for this problem\nis an unobtrusive biometric setting in which a user is observed during access\nto a document, but no specific challenge protocol requiring the user's time and\nattention is carried out. Existing models of individual differences in gaze\ncontrol during reading are either based on simple aggregate features of eye\nmovements, or rely on parametric density models to describe, for instance,\nsaccade amplitudes or word fixation durations. We develop flexible\nsemiparametric models of eye movements during reading in which densities are\ninferred under a Gaussian process prior centered at a parametric distribution\nfamily that is expected to approximate the true distribution well. An empirical\nstudy on reading data from 251 individuals shows significant improvements over\nthe state of the art.\n"
p21
aS' We propose to enhance the RNN decoder in a neural machine translator (NMT)\nwith external memory, as a natural but powerful extension to the state in the\ndecoding RNN. This memory-enhanced RNN decoder is called \\textsc{MemDec}. At\neach time during decoding, \\textsc{MemDec} will read from this memory and write\nto this memory once, both with content-based addressing. Unlike the unbounded\nmemory in previous work\\cite{RNNsearch} to store the representation of source\nsentence, the memory in \\textsc{MemDec} is a matrix with pre-determined size\ndesigned to better capture the information important for the decoding process\nat each time step. Our empirical study on Chinese-English translation shows\nthat it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses,\nyielding the best performance achieved with the same training set.\n'
p22
aS' When humans read text, they fixate some words and skip others. However, there\nhave been few attempts to explain skipping behavior with computational models,\nas most existing work has focused on predicting reading times (e.g.,~using\nsurprisal). In this paper, we propose a novel approach that models both\nskipping and reading, using an unsupervised architecture that combines a neural\nattention with autoencoding, trained on raw text using reinforcement learning.\nOur model explains human reading behavior as a tradeoff between precision of\nlanguage understanding (encoding the input accurately) and economy of attention\n(fixating as few words as possible). We evaluate the model on the Dundee\neye-tracking corpus, showing that it accurately predicts skipping behavior and\nreading times, is competitive with surprisal, and captures known qualitative\nfeatures of human reading.\n'
p23
aS' Stance detection is the task of classifying the attitude expressed in a text\ntowards a target such as Hillary Clinton to be "positive", negative" or\n"neutral". Previous work has assumed that either the target is mentioned in the\ntext or that training data for every target is given. This paper considers the\nmore challenging version of this task, where targets are not always mentioned\nand no training data is available for the test targets. We experiment with\nconditional LSTM encoding, which builds a representation of the tweet that is\ndependent on the target, and demonstrate that it outperforms encoding the tweet\nand the target independently. Performance is improved further when the\nconditional model is augmented with bidirectional encoding. We evaluate our\napproach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving\nperformance second best only to a system trained on semi-automatically labelled\ntweets for the test target. When such weak supervision is added, our approach\nachieves state-of-the-art results.\n'
p24
aS' Most successful information extraction systems operate with access to a large\ncollection of documents. In this work, we explore the task of acquiring and\nincorporating external evidence to improve extraction accuracy in domains where\nthe amount of training data is scarce. This process entails issuing search\nqueries, extraction from new sources and reconciliation of extracted values,\nwhich are repeated until sufficient evidence is collected. We approach the\nproblem using a reinforcement learning framework where our model learns to\nselect optimal actions based on contextual information. We employ a deep\nQ-network, trained to optimize a reward function that reflects extraction\naccuracy while penalizing extra effort. Our experiments on two databases -- of\nshooting incidents, and food adulteration cases -- demonstrate that our system\nsignificantly outperforms traditional extractors and a competitive\nmeta-classifier baseline.\n'
p25
aS' Verbs play a critical role in the meaning of sentences, but these ubiquitous\nwords have received little attention in recent distributional semantics\nresearch. We introduce SimVerb-3500, an evaluation resource that provides human\nratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed\nverb types from the USF free-association database, providing at least three\nexamples for every VerbNet class. This broad coverage facilitates detailed\nanalyses of how syntactic and semantic phenomena together influence human\nunderstanding of verb meaning. Further, with significantly larger development\nand test sets than existing benchmarks, SimVerb-3500 enables more robust\nevaluation of representation learning architectures and promotes the\ndevelopment of methods tailored to verbs. We hope that SimVerb-3500 will enable\na richer understanding of the diversity and complexity of verb semantics and\nguide the development of systems that can effectively represent and interpret\nthis meaning.\n'
p26
aS' We propose a neural-network based model for coordination boundary prediction.\nThe network is designed to incorporate two signals: the similarity between\nconjuncts and the observation that replacing the whole coordination phrase with\na conjunct tends to produce a coherent sentences. The modeling makes use of\nseveral LSTM networks. The model is trained solely on conjunction annotations\nin a Treebank, without using external resources. We show improvements on\npredicting coordination boundaries on the PTB compared to two state-of-the-art\nparsers; as well as improvement over previous coordination boundary prediction\nsystems on the Genia corpus.\n'
p27
aS' Implicit discourse relation recognition is a crucial component for automatic\ndiscourselevel analysis and nature language understanding. Previous studies\nexploit discriminative models that are built on either powerful manual features\nor deep discourse representations. In this paper, instead, we explore\ngenerative models and propose a variational neural discourse relation\nrecognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed\nprobabilistic model with a latent continuous variable that generates both a\ndiscourse and the relation between the two arguments of the discourse. In order\nto perform efficient inference and learning, we introduce neural discourse\nrelation models to approximate the prior and posterior distributions of the\nlatent variable, and employ these approximated distributions to optimize a\nreparameterized variational lower bound. This allows VarNDRR to be trained with\nstandard stochastic gradient methods. Experiments on the benchmark data set\nshow that VarNDRR can achieve comparable results against stateof- the-art\nbaselines without using any manual features.\n'
p28
aS' Recent neural models of dialogue generation offer great promise for\ngenerating responses for conversational agents, but tend to be shortsighted,\npredicting utterances one at a time while ignoring their influence on future\noutcomes. Modeling the future direction of a dialogue is crucial to generating\ncoherent, interesting dialogues, a need which led traditional NLP models of\ndialogue to draw on reinforcement learning. In this paper, we show how to\nintegrate these goals, applying deep reinforcement learning to model future\nreward in chatbot dialogue. The model simulates dialogues between two virtual\nagents, using policy gradient methods to reward sequences that display three\nuseful conversational properties: informativity (non-repetitive turns),\ncoherence, and ease of answering (related to forward-looking function). We\nevaluate our model on diversity, length as well as with human judges, showing\nthat the proposed algorithm generates more interactive responses and manages to\nfoster a more sustained conversation in dialogue simulation. This work marks a\nfirst step towards learning a neural conversational model based on the\nlong-term success of dialogues.\n'
p29
aS' Directly reading documents and being able to answer questions from them is an\nunsolved challenge. To avoid its inherent difficulty, question answering (QA)\nhas been directed towards using Knowledge Bases (KBs) instead, which has proven\neffective. Unfortunately KBs often suffer from being too restrictive, as the\nschema cannot support certain types of answers, and too sparse, e.g. Wikipedia\ncontains much more information than Freebase. In this work we introduce a new\nmethod, Key-Value Memory Networks, that makes reading documents more viable by\nutilizing different encodings in the addressing and output stages of the memory\nread operation. To compare using KBs, information extraction or Wikipedia\ndocuments directly in a single framework we construct an analysis tool,\nWikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in\nthe domain of movies. Our method reduces the gap between all three settings. It\nalso achieves state-of-the-art results on the existing WikiQA benchmark.\n'
p30
aS' In this work, we study parameter tuning towards the M^2 metric, the standard\nmetric for automatic grammar error correction (GEC) tasks. After implementing\nM^2 as a scorer in the Moses tuning framework, we investigate interactions of\ndense and sparse features, different optimizers, and tuning strategies for the\nCoNLL-2014 shared task. We notice erratic behavior when optimizing sparse\nfeature weights with M^2 and offer partial solutions. We find that a bare-bones\nphrase-based SMT setup with task-specific parameter-tuning outperforms all\npreviously published results for the CoNLL-2014 test set by a large margin\n(46.37% M^2 over previously 41.75%, by an SMT system with neural features)\nwhile being trained on the same, publicly available data. Our newly introduced\ndense and sparse features widen that gap, and we improve the state-of-the-art\nto 49.49% M^2.\n'
p31
aS' Situated question answering is the problem of answering questions about an\nenvironment such as an image or diagram. This problem requires jointly\ninterpreting a question and an environment using background knowledge to select\nthe correct answer. We present Parsing to Probabilistic Programs (P3), a novel\nsituated question answering model that can use background knowledge and global\nfeatures of the question/environment interpretation while retaining efficient\napproximate inference. Our key insight is to treat semantic parses as\nprobabilistic programs that execute nondeterministically and whose possible\nexecutions represent environmental uncertainty. We evaluate our approach on a\nnew, publicly-released data set of 5000 science diagram questions,\noutperforming several competitive classical and neural baselines.\n'
p32
aS' Word embeddings allow natural language processing systems to share\nstatistical information across related words. These embeddings are typically\nbased on distributional statistics, making it difficult for them to generalize\nto rare or unseen words. We propose to improve word embeddings by incorporating\nmorphological information, capturing shared sub-word features. Unlike previous\nwork that constructs word embeddings directly from morphemes, we combine\nmorphological and distributional information in a unified probabilistic\nframework, in which the word embedding is a latent variable. The morphological\ninformation provides a prior distribution on the latent word embeddings, which\nin turn condition a likelihood function over an observed corpus. This approach\nyields improvements on intrinsic word similarity evaluations, and also in the\ndownstream task of part-of-speech tagging.\n'
p33
aS' Neural machine translation (NMT) often makes mistakes in translating\nlow-frequency content words that are essential to understanding the meaning of\nthe sentence. We propose a method to alleviate this problem by augmenting NMT\nsystems with discrete translation lexicons that efficiently encode translations\nof these low-frequency words. We describe a method to calculate the lexicon\nprobability of the next word in the translation candidate by using the\nattention vector of the NMT model to select which source word lexical\nprobabilities the model should focus on. We test two methods to combine this\nprobability with the standard NMT probability: (1) using it as a bias, and (2)\nlinear interpolation. Experiments on two corpora show an improvement of 2.0-2.3\nBLEU and 0.13-0.44 NIST score, and faster convergence time.\n'
p34
aS' Distributional models are derived from co-occurrences in a corpus, where only\na small proportion of all possible plausible co-occurrences will be observed.\nThis results in a very sparse vector space, requiring a mechanism for inferring\nmissing knowledge. Most methods face this challenge in ways that render the\nresulting word representations uninterpretable, with the consequence that\nsemantic composition becomes hard to model. In this paper we explore an\nalternative which involves explicitly inferring unobserved co-occurrences using\nthe distributional neighbourhood. We show that distributional inference\nimproves sparse word representations on several word similarity benchmarks and\ndemonstrate that our model is competitive with the state-of-the-art for\nadjective-noun, noun-noun and verb-object compositions while being fully\ninterpretable.\n'
p35
aS' We introduce two first-order graph-based dependency parsers achieving a new\nstate of the art. The first is a consensus parser built from an ensemble of\nindependently trained greedy LSTM transition-based parsers with different\nrandom initializations. We cast this approach as minimum Bayes risk decoding\n(under the Hamming cost) and argue that weaker consensus within the ensemble is\na useful signal of difficulty or ambiguity. The second parser is a\n"distillation" of the ensemble into a single model. We train the distillation\nparser using a structured hinge loss objective with a novel cost that\nincorporates ensemble uncertainty estimates for each possible attachment,\nthereby avoiding the intractable cross-entropy computations required by\napplying standard distillation objectives to problems with structured outputs.\nThe first-order distillation parser matches or surpasses the state of the art\non English, Chinese, and German.\n'
p36
aS' We develop a novel bi-directional attention model for dependency parsing,\nwhich learns to agree on headword predictions from the forward and backward\nparsing directions. The parsing procedure for each direction is formulated as\nsequentially querying the memory component that stores continuous headword\nembeddings. The proposed parser makes use of {\\it soft} headword embeddings,\nallowing the model to implicitly capture high-order parsing history without\ndramatically increasing the computational complexity. We conduct experiments on\nEnglish, Chinese, and 12 other languages from the CoNLL 2006 shared task,\nshowing that the proposed model achieves state-of-the-art unlabeled attachment\nscores on 6 languages.\n'
p37
aS' We present a novel semi-supervised approach for sequence transduction and\napply it to semantic parsing. The unsupervised component is based on a\ngenerative model in which latent sentences generate the unpaired logical forms.\nWe apply this method to a number of semantic parsing tasks focusing on domains\nwith limited access to labelled training data and extend those datasets with\nsynthetically generated logical forms.\n'
p38
aS' We introduce an online neural sequence to sequence model that learns to\nalternate between encoding and decoding segments of the input as it is read. By\nindependently tracking the encoding and decoding representations our algorithm\npermits exact polynomial marginalization of the latent segmentation during\ntraining, and during decoding beam search is employed to find the best\nalignment path together with the predicted output sequence. Our model tackles\nthe bottleneck of vanilla encoder-decoders that have to read and memorize the\nentire input sequence in their fixed-length hidden states before producing any\noutput. It is different from previous attentive models in that, instead of\ntreating the attention weights as output of a deterministic function, our model\nassigns attention weights to a sequential latent variable which can be\nmarginalized out and permits online generation. Experiments on abstractive\nsentence summarization and morphological inflection show significant\nperformance gains over the baseline encoder-decoders.\n'
p39
aS' The encoder-decoder framework for neural machine translation (NMT) has been\nshown effective in large data scenarios, but is much less effective for\nlow-resource languages. We present a transfer learning method that\nsignificantly improves Bleu scores across a range of low-resource languages.\nOur key idea is to first train a high-resource language pair (the parent\nmodel), then transfer some of the learned parameters to the low-resource pair\n(the child model) to initialize and constrain training. Using our transfer\nlearning method we improve baseline NMT models by an average of 5.6 Bleu on\nfour low-resource language pairs. Ensembling and unknown word replacement add\nanother 2 Bleu which brings the NMT performance on low-resource machine\ntranslation close to a strong syntax based machine translation (SBMT) system,\nexceeding its performance on one language pair. Additionally, using the\ntransfer learning model for re-scoring, we can improve the SBMT system by an\naverage of 1.3 Bleu, improving the state-of-the-art on low-resource machine\ntranslation.\n'
p40
aS' Methods based on representation learning currently hold the state-of-the-art\nin many natural language processing and knowledge base inference tasks. Yet, a\nmajor challenge is how to efficiently incorporate commonsense knowledge into\nsuch models. A recent approach regularizes relation and entity representations\nby propositionalization of first-order logic rules. However,\npropositionalization does not scale beyond domains with only few entities and\nrules. In this paper we present a highly efficient method for incorporating\nimplication rules into distributed representations for automated knowledge base\nconstruction. We map entity-tuple embeddings into an approximately Boolean\nspace and encourage a partial ordering over relation embeddings based on\nimplication rules mined from WordNet. Surprisingly, we find that the strong\nrestriction of the entity-tuple embedding space does not hurt the\nexpressiveness of the model and even acts as a regularizer that improves\ngeneralization. By incorporating few commonsense rules, we achieve an increase\nof 2 percentage points mean average precision over a matrix factorization\nbaseline, while observing a negligible increase in runtime.\n'
p41
aS' The problem of accurately predicting relative reading difficulty across a set\nof sentences arises in a number of important natural language applications,\nsuch as finding and curating effective usage examples for intelligent language\ntutoring systems. Yet while significant research has explored document- and\npassage-level reading difficulty, the special challenges involved in assessing\naspects of readability for single sentences have received much less attention,\nparticularly when considering the role of surrounding passages. We introduce\nand evaluate a novel approach for estimating the relative reading difficulty of\na set of sentences, with and without surrounding context. Using different sets\nof lexical and grammatical features, we explore models for predicting pairwise\nrelative difficulty using logistic regression, and examine rankings generated\nby aggregating pairwise difficulty labels using a Bayesian rating system to\nform a final ranking. We also compare rankings derived for sentences assessed\nwith and without context, and find that contextual features can help predict\ndifferences in relative difficulty judgments across these two conditions.\n'
p42
aS' We introduce the first global recursive neural parsing model with optimality\nguarantees during decoding. To support global features, we give up dynamic\nprograms and instead search directly in the space of all possible subtrees.\nAlthough this space is exponentially large in the sentence length, we show it\nis possible to learn an efficient A* parser. We augment existing parsing\nmodels, which have informative bounds on the outside score, with a global model\nthat has loose bounds but only needs to model non-local phenomena. The global\nmodel is trained with a new objective that encourages the parser to explore a\ntiny fraction of the search space. The approach is applied to CCG parsing,\nimproving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal\nparse for 99.9% of held-out sentences, exploring on average only 190 subtrees.\n'
p43
aS' Prediction without justification has limited applicability. As a remedy, we\nlearn to extract pieces of input text as justifications -- rationales -- that\nare tailored to be short and coherent, yet sufficient for making the same\nprediction. Our approach combines two modular components, generator and\nencoder, which are trained to operate well together. The generator specifies a\ndistribution over text fragments as candidate rationales and these are passed\nthrough the encoder for prediction. Rationales are never given during training.\nInstead, the model is regularized by desiderata for rationales. We evaluate the\napproach on multi-aspect sentiment analysis against manually annotated test\ncases. Our approach outperforms attention-based baseline by a significant\nmargin. We also successfully illustrate the method on the question retrieval\ntask.\n'
p44
aS" For many low-resource languages, spoken language resources are more likely to\nbe annotated with translations than with transcriptions. Translated speech data\nis potentially valuable for documenting endangered languages or for training\nspeech translation systems. A first step towards making use of such data would\nbe to automatically align spoken words with their translations. We present a\nmodel that combines Dyer et al.'s reparameterization of IBM Model 2\n(fast-align) and k-means clustering using Dynamic Time Warping as a distance\nmetric. The two components are trained jointly using expectation-maximization.\nIn an extremely low-resource scenario, our model performs significantly better\nthan both a neural model and a strong baseline.\n"
p45
aS' Though dialectal language is increasingly abundant on social media, few\nresources exist for developing NLP tools to handle such language. We conduct a\ncase study of dialectal language in online conversational text by investigating\nAfrican-American English (AAE) on Twitter. We propose a distantly supervised\nmodel to identify AAE-like language from demographics associated with\ngeo-located messages, and we verify that this language follows well-known AAE\nlinguistic phenomena. In addition, we analyze the quality of existing language\nidentification and dependency parsing tools on AAE-like text, demonstrating\nthat they perform poorly on such text compared to text associated with white\nspeakers. We also provide an ensemble classifier for language identification\nwhich eliminates this disparity and release a new corpus of tweets containing\nAAE-like language.\n'
p46
aS" We present the EpiReader, a novel model for machine comprehension of text.\nMachine comprehension of unstructured, real-world text is a major research goal\nfor natural language processing. Current tests of machine comprehension pose\nquestions whose answers can be inferred from some supporting text, and evaluate\na model's response to the questions. The EpiReader is an end-to-end neural\nmodel comprising two components: the first component proposes a small set of\ncandidate answers after comparing a question to its supporting text, and the\nsecond component formulates hypotheses using the proposed candidates and the\nquestion, then reranks the hypotheses based on their estimated concordance with\nthe supporting text. We present experiments demonstrating that the EpiReader\nsets a new state-of-the-art on the CNN and Children's Book Test machine\ncomprehension benchmarks, outperforming previous neural models by a significant\nmargin.\n"
p47
aS' Neural encoder-decoder models have shown great success in many sequence\ngeneration tasks. However, previous work has not investigated situations in\nwhich we would like to control the length of encoder-decoder outputs. This\ncapability is crucial for applications such as text summarization, in which we\nhave to generate concise summaries with a desired length. In this paper, we\npropose methods for controlling the output sequence length for neural\nencoder-decoder models: two decoding-based methods and two learning-based\nmethods. Results show that our learning-based methods have the capability to\ncontrol length without degrading summary quality in a summarization task.\n'
p48
aS' Neural machine translation (NMT) offers a novel alternative formulation of\ntranslation that is potentially simpler than statistical approaches. However to\nreach competitive performance, NMT models need to be exceedingly large. In this\npaper we consider applying knowledge distillation approaches (Bucila et al.,\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\nneural models in other domains to the problem of NMT. We demonstrate that\nstandard knowledge distillation applied to word-level prediction can be\neffective for NMT, and also introduce two novel sequence-level versions of\nknowledge distillation that further improve performance, and somewhat\nsurprisingly, seem to eliminate the need for beam search (even when applied on\nthe original teacher model). Our best student model runs 10 times faster than\nits state-of-the-art teacher with little loss in performance. It is also\nsignificantly better than a baseline model trained without knowledge\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\npruning on top of knowledge distillation results in a student model that has 13\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\nBLEU.\n'
p49
aS' In this paper, we propose a novel finetuning algorithm for the recently\nintroduced multi-way, mulitlingual neural machine translate that enables\nzero-resource machine translation. When used together with novel many-to-one\ntranslation strategies, we empirically show that this finetuning algorithm\nallows the multi-way, multilingual model to translate a zero-resource language\npair (1) as well as a single-pair neural translation model trained with up to\n1M direct parallel sentences of the same language pair and (2) better than\npivot-based translation strategy, while keeping only one additional copy of\nattention-related parameters.\n'
p50
aS' We show that a character-level encoder-decoder framework can be successfully\napplied to question answering with a structured knowledge base. We use our\nmodel for single-relation question answering and demonstrate the effectiveness\nof our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we\nimprove state-of-the-art accuracy from 63.9% to 70.9%, without use of\nensembles. Importantly, our character-level model has 16x fewer parameters than\nan equivalent word-level model, can be learned with significantly less data\ncompared to previous work, which relies on data augmentation, and is robust to\nnew entities in testing.\n'
p51
aS' Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.\n'
p52
aS' Encoder-decoder networks are popular for modeling sequences probabilistically\nin many applications. These models use the power of the Long Short-Term Memory\n(LSTM) architecture to capture the full dependence among variables, unlike\nearlier models like CRFs that typically assumed conditional independence among\nnon-adjacent variables. However in practice encoder-decoder models exhibit a\nbias towards short sequences that surprisingly gets worse with increasing beam\nsize.\n'
p53
aS' Recently, a number of deep-learning based models have been proposed for the\ntask of Visual Question Answering (VQA). The performance of most models is\nclustered around 60-70%. In this paper we propose systematic methods to analyze\nthe behavior of these models as a first step towards recognizing their\nstrengths and weaknesses, and identifying the most fruitful directions for\nprogress. We analyze two models, one each from two major classes of VQA models\n-- with-attention and without-attention and show the similarities and\ndifferences in the behavior of these models. We also analyze the winning entry\nof the VQA Challenge 2016.\n'
p54
aS' This paper investigates how linguistic knowledge mined from large text\ncorpora can aid the generation of natural language descriptions of videos.\nSpecifically, we integrate both a neural language model and distributional\nsemantics trained on large text corpora into a recent LSTM-based architecture\nfor video description. We evaluate our approach on a collection of Youtube\nvideos as well as two large movie description datasets showing significant\nimprovements in grammaticality while maintaining or modestly improving\ndescriptive quality. Further, we show that such techniques can be beneficial\nfor describing unseen object classes with no paired training data (zeroshot\ncaptioning).\n'
p55
aS' Recurrent Neural Network (RNN) and one of its specific architectures, Long\nShort-Term Memory (LSTM), have been widely used for sequence labeling. In this\npaper, we first enhance LSTM-based sequence labeling to explicitly model label\ndependencies. Then we propose another enhancement to incorporate the global\ninformation spanning over the whole input sequence. The latter proposed method,\nencoder-labeler LSTM, first encodes the whole input sequence into a fixed\nlength vector with the encoder LSTM, and then uses this encoded vector as the\ninitial state of another LSTM for sequence labeling. Combining these methods,\nwe can predict the label sequence with considering label dependencies and\ninformation of whole input sequence. In the experiments of a slot filling task,\nwhich is an essential component of natural language understanding, with using\nthe standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.\n'
p56
aS' This paper investigates the effects of data size and frequency range on\ndistributional semantic models. We compare the performance of a number of\nrepresentative models for several test settings over data of varying sizes, and\nover test items of various frequency. Our results show that neural\nnetwork-based models underperform when the data is small, and that the most\nreliable model over data of varying sizes and frequency ranges is the inverted\nfactorized model.\n'
p57
aS' In this paper, we improve the attention or alignment accuracy of neural\nmachine translation by utilizing the alignments of training sentence pairs. We\nsimply compute the distance between the machine attentions and the "true"\nalignments, and minimize this cost in the training procedure. Our experiments\non large-scale Chinese-to-English task show that our model improves both\ntranslation and alignment qualities significantly over the large-vocabulary\nneural machine translation system, and even beats a state-of-the-art\ntraditional syntax-based system.\n'
p58
aS' This paper explores the task of translating natural language queries into\nregular expressions which embody their meaning. In contrast to prior work, the\nproposed neural model does not utilize domain-specific crafting, learning to\ntranslate directly from a parallel corpus. To fully explore the potential of\nneural models, we propose a methodology for collecting a large corpus of\nregular expression, natural language pairs. Our resulting model achieves a\nperformance gain of 19.6% over previous state-of-the-art models.\n'
p59
aS' Several studies on sentence processing suggest that the mental lexicon keeps\ntrack of the mutual expectations between words. Current DSMs, however,\nrepresent context words as separate features, thereby loosing important\ninformation for word expectations, such as word interrelations. In this paper,\nwe present a DSM that addresses this issue by defining verb contexts as joint\nsyntactic dependencies. We test our representation in a verb similarity task on\ntwo datasets, showing that joint contexts achieve performances comparable to\nsingle dependencies or even better. Moreover, they are able to overcome the\ndata sparsity problem of joint feature spaces, in spite of the limited size of\nour training corpus.\n'
p60
aS' The production of color language is essential for grounded language\ngeneration. Color descriptions have many challenging properties: they can be\nvague, compositionally complex, and denotationally rich. We present an\neffective approach to generating color descriptions using recurrent neural\nnetworks and a Fourier-transformed color representation. Our model outperforms\nprevious work on a conditional language modeling task over a large corpus of\nnaturalistic color descriptions. In addition, probing the model\'s output\nreveals that it can accurately produce not only basic color terms but also\ndescriptors with non-convex denotations ("greenish"), bare modifiers ("bright",\n"dull"), and compositional phrases ("faded teal") not seen in training.\n'
p61
aS" Opinion mining from customer reviews has become pervasive in recent years.\nSentences in reviews, however, are usually classified independently, even\nthough they form part of a review's argumentative structure. Intuitively,\nsentences in a review build and elaborate upon each other; knowledge of the\nreview structure and sentential context should thus inform the classification\nof each sentence. We demonstrate this hypothesis for the task of aspect-based\nsentiment analysis by modeling the interdependencies of sentences in a review\nwith a hierarchical bidirectional LSTM. We show that the hierarchical model\noutperforms two non-hierarchical baselines, obtains results competitive with\nthe state-of-the-art, and outperforms the state-of-the-art on five\nmultilingual, multi-domain datasets without any hand-engineered features or\nexternal resources.\n"
p62
aS' Deep neural networks have achieved remarkable results across many language\nprocessing tasks, however these methods are highly sensitive to noise and\nadversarial attacks. We present a regularization based method for limiting\nnetwork sensitivity to its inputs, inspired by ideas from computer vision, thus\nlearning models that are more robust. Empirical evaluation over a range of\nsentiment datasets with a convolutional neural network shows that, compared to\na baseline model and the dropout method, our method achieves superior\nperformance over noisy inputs and out-of-domain data.\n'
p63
aS' We explore the use of the orthographic syllable, a variable-length\nconsonant-vowel sequence, as a basic unit of translation between related\nlanguages which use abugida or alphabetic scripts. We show that orthographic\nsyllable level translation significantly outperforms models trained over other\nbasic units (word, morpheme and character) when training over small parallel\ncorpora.\n'
p64
aS' We have constructed a new "Who-did-What" dataset of over 200,000\nfill-in-the-gap (cloze) multiple choice reading comprehension problems\nconstructed from the LDC English Gigaword newswire corpus. The WDW dataset has\na variety of novel features. First, in contrast with the CNN and Daily Mail\ndatasets (Hermann et al., 2015) we avoid using article summaries for question\nformation. Instead, each problem is formed from two independent articles --- an\narticle given as the passage to be read and a separate article on the same\nevents used to form the question. Second, we avoid anonymization --- each\nchoice is a person named entity. Third, the problems have been filtered to\nremove a fraction that are easily solved by simple baselines, while remaining\n84% solvable by humans. We report performance benchmarks of standard systems\nand propose the WDW dataset as a challenge task for the community.\n'
p65
aS' Semantic error detection and correction is an important task for applications\nsuch as fact checking, speech-to-text or grammatical error correction. Current\napproaches generally focus on relatively shallow semantics and do not account\nfor numeric quantities. Our approach uses language models grounded in numbers\nwithin the text. Such groundings are easily achieved for recurrent neural\nlanguage model architectures, which can be further conditioned on incomplete\nbackground knowledge bases. Our evaluation on clinical reports shows that\nnumerical grounding improves perplexity by 33% and F1 for semantic error\ncorrection by 5 points when compared to ungrounded approaches. Conditioning on\na knowledge base yields further improvements.\n'
p66
aS' The task of AMR-to-text generation is to generate grammatical text that\nsustains the semantic meaning for a given AMR graph. We at- tack the task by\nfirst partitioning the AMR graph into smaller fragments, and then generating\nthe translation for each fragment, before finally deciding the order by solving\nan asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy\nclassifier is trained to estimate the traveling costs, and a TSP solver is used\nto find the optimized solution. The final model reports a BLEU score of 22.44\non the SemEval-2016 Task8 dataset.\n'
p67
aS' We introduce a recurrent neural network language model (RNN-LM) with long\nshort-term memory (LSTM) units that utilizes both character-level and\nword-level inputs. Our model has a gate that adaptively finds the optimal\nmixture of the character-level and word-level inputs. The gate creates the\nfinal vector representation of a word by combining two distinct representations\nof the word. The character-level inputs are converted into vector\nrepresentations of words using a bidirectional LSTM. The word-level inputs are\nprojected into another high-dimensional space by a word lookup table. The final\nvector representations of words are used in the LSTM language model which\npredicts the next word given all the preceding words. Our model with the gating\nmechanism effectively utilizes the character-level inputs for rare and\nout-of-vocabulary words and outperforms word-level language models on several\nEnglish corpora.\n'
p68
aS' We propose a simple neural architecture for natural language inference. Our\napproach uses attention to decompose the problem into subproblems that can be\nsolved separately, thus making it trivially parallelizable. On the Stanford\nNatural Language Inference (SNLI) dataset, we obtain state-of-the-art results\nwith almost an order of magnitude fewer parameters than previous work and\nwithout relying on any word-order information. Adding intra-sentence attention\nthat takes a minimum amount of order into account yields further improvements.\n'
p69
aS' Recent work on word ordering has argued that syntactic structure is\nimportant, or even required, for effectively recovering the order of a\nsentence. We find that, in fact, an n-gram language model with a simple\nheuristic gives strong results on this task. Furthermore, we show that a long\nshort-term memory (LSTM) language model is even more effective at recovering\norder, with our basic model outperforming a state-of-the-art syntactic model by\n11.5 BLEU points. Additional data and larger beams yield further gains, at the\nexpense of training and search time.\n'
p70
aS' This work investigates style and topic aspects of language in online\ncommunities: looking at both utility as an identifier of the community and\ncorrelation with community reception of content. Style is characterized using a\nhybrid word and part-of-speech tag n-gram language model, while topic is\nrepresented using Latent Dirichlet Allocation. Experiments with several Reddit\nforums show that style is a better indicator of community identity than topic,\neven for communities organized around specific topics. Further, there is a\npositive correlation between the community reception to a contribution and the\nstyle similarity to that community, but not so for topic similarity.\n'
p71
aS' We present an interpretable neural network approach to predicting and\nunderstanding politeness in natural language requests. Our models are based on\nsimple convolutional neural networks directly on raw text, avoiding any manual\nidentification of complex sentiment or syntactic features, while performing\nbetter than such feature-based models from previous work. More importantly, we\nuse the challenging task of politeness prediction as a testbed to next present\na much-needed understanding of what these successful networks are actually\nlearning. For this, we present several network visualizations based on\nactivation clusters, first derivative saliency, and embedding space\ntransformations, helping us automatically identify several subtle linguistics\nmarkers of politeness theories. Further, this analysis reveals multiple novel,\nhigh-scoring politeness strategies which, when added back as new features,\nreduce the accuracy gap between the original featurized system and the neural\nmodel, thus providing a clear quantitative interpretation of the success of\nthese neural networks.\n'
p72
aS' Coreference resolution systems are typically trained with heuristic loss\nfunctions that require careful tuning. In this paper we instead apply\nreinforcement learning to directly optimize a neural mention-ranking model for\ncoreference evaluation metrics. We experiment with two approaches: the\nREINFORCE policy gradient algorithm and a reward-rescaled max-margin objective.\nWe find the latter to be more effective, resulting in significant improvements\nover the current state-of-the-art on the English and Chinese portions of the\nCoNLL 2012 Shared Task.\n'
p73
aS' We compare the effectiveness of four different syntactic CCG parsers for a\nsemantic slot-filling task to explore how much syntactic supervision is\nrequired for downstream semantic analysis. This extrinsic, task-based\nevaluation also provides a unique window into the semantics captured (or\nmissed) by unsupervised grammar induction systems.\n'
p74
aS" We suggest a compositional vector representation of parse trees that relies\non a recursive combination of recurrent-neural network encoders. To demonstrate\nits effectiveness, we use the representation as the backbone of a greedy,\nbottom-up dependency parser, achieving state-of-the-art accuracies for English\nand Chinese, without relying on external word embeddings. The parser's\nimplementation is available for download at the first author's webpage.\n"
p75
aS" Neural machine translation (NMT) aims at solving machine translation (MT)\nproblems using neural networks and has exhibited promising results in recent\nyears. However, most of the existing NMT models are shallow and there is still\na performance gap between a single NMT model and the best conventional MT\nsystem. In this work, we introduce a new type of linear connections, named\nfast-forward connections, based on deep Long Short-Term Memory (LSTM) networks,\nand an interleaved bi-directional architecture for stacking the LSTM layers.\nFast-forward connections play an essential role in propagating the gradients\nand building a deep topology of depth 16. On the WMT'14 English-to-French task,\nwe achieve BLEU=37.7 with a single attention model, which outperforms the\ncorresponding single shallow model by 6.2 BLEU points. This is the first time\nthat a single NMT model achieves state-of-the-art performance and outperforms\nthe best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3\neven without using an attention mechanism. After special handling of unknown\nwords and model ensembling, we obtain the best score reported to date on this\ntask with BLEU=40.4. Our models are also validated on the more difficult WMT'14\nEnglish-to-German task.\n"
p76
aS' Mental illness is one of the most pressing public health issues of our time.\nWhile counseling and psychotherapy can be effective treatments, our knowledge\nabout how to conduct successful counseling conversations has been limited due\nto lack of large-scale data with labeled outcomes of the conversations. In this\npaper, we present a large-scale, quantitative study on the discourse of\ntext-message-based counseling conversations. We develop a set of novel\ncomputational discourse analysis methods to measure how various linguistic\naspects of conversations are correlated with conversation outcomes. Applying\ntechniques such as sequence-based conversation models, language model\ncomparisons, message clustering, and psycholinguistics-inspired word frequency\nanalyses, we discover actionable conversation strategies that are associated\nwith better conversation outcomes.\n'
p77
a.